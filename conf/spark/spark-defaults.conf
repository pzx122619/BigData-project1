# Configure Spark on YARN
spark.master=yarn

# Dynamic allocation on YARN
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=1
spark.executor.instances=3
spark.dynamicAllocation.maxExecutors=6
spark.shuffle.service.enabled=true
spark.scheduler.minRegisteredResourcesRatio=0.0
spark.dynamicAllocation.executorIdleTimeout=60s
spark.dynamicAllocation.shuffleTracking.timeout=300s

spark.rpc.message.maxSize=512

# Enable using Hive as the metastore for Spark
spark.sql.catalogImplementation=hive
spark.checkpoint.compress=true
spark.sql.hive.metastore.version=4.0.1
spark.hadoop.hive.metastore.uris=thrift://master:9083
spark.sql.hive.metastore.jars=file:///opt/hive/lib/*
spark.sql.warehouse.dir=hdfs://master:8020/user/hive/warehouse

# Enable event logging for Spark History Server
spark.eventLog.enabled=true
spark.eventLog.dir=hdfs://master:8020/user/spark/logs

# Set the history server log directory
spark.history.fs.logDirectory=hdfs://master:8020/user/spark/logs

# Spark History Server
spark.history.ui.port=18080
spark.yarn.historyServer.address=http://master:18080

# AM
spark.yarn.am.memory=640m

# Driver
spark.driver.memory=2g
spark.driver.cores=1

# Executor
spark.executor.memory=1g
spark.executor.cores=1

# Additional configs as needed...
spark.yarn.jars=file:///opt/spark/jars/*

# Biblioteki natywne
spark.executor.extraLibraryPath=/opt/hadoop/lib/native
spark.driver.extraLibraryPath=/opt/hadoop/lib/native

# On clusters without Kerberos integration, use unmanaged AM to speed-up Spark
spark.yarn.unmanagedAM.enabled=true

# Enable adaptive query execution, which re-optimizes the query plan
# in the middle of query execution, based on accurate runtime statistics.
spark.sql.adaptive.enabled=true

# Reorder joins when cost-based optimization enabled to improve performance.
spark.sql.cbo.joinReorder.enabled=true

spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2

spark.sql.shuffle.partitions=10

# Adding namespace to extract app_name and app_id for spark metrics
spark.metrics.namespace=app_name##${spark.app.name}.app_id##${spark.app.id}


